{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disen 3 to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_session 81516\n",
      "#train_items 14650\n",
      "#test_session:  8976\n",
      "no data augment\n",
      "no data augment\n",
      "train sequence:  [[1], [3], [4], [6, 7, 8], [10, 11, 12, 13, 14, 15, 16]]\n",
      "train lab:  [2, 3, 5, 9, 17]\n",
      "item without co-occurren:  85\n",
      "id, train, co-occurence sess:  38771\n",
      "Modality, train, non co-occurence sess:  42745\n",
      "id, test, co-occurence sess:  38771\n",
      "Modality, test, non co-occurence sess:  42745\n",
      "#train_interactions:  253927\n",
      "#test_interactions:  28175\n",
      "#train_session:  81516\n",
      "#test_session:  8976\n",
      "sequence average length:  3.1174247447288157\n",
      "dataset:  Sports_and_Outdoors\n",
      "2023-08-20 18:33:34\n",
      "data formulation: id_seq,labs\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Sports_and_Outdoors'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "num_pos = 10\n",
    "num_neg = 10\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() \n",
    "\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "\n",
    "item_dict = {} #dict(old_itemID: new_itemID)\n",
    "\n",
    "\n",
    "\n",
    "# tra_sess tra_price tra_cate\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    global item_ctr\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            print('session length is 1')\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "    print(\"#train_session\",len(train_seqs))\n",
    "    print(\"#train_items\",item_ctr-1)\n",
    "    return train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        outseq = []\n",
    "        for i in tes_sess[s]:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            print('obtain test session length is 1')\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    print(\"#test_session: \",len(test_seqs))\n",
    "    return test_seqs\n",
    "\n",
    "# Convert test sessions to sequences, renumbering items that do not appear in training set\n",
    "def obtian_tes_cold():\n",
    "    test_seqs = []\n",
    "    num_cold = 0\n",
    "    global item_ctr\n",
    "#     test_cate = []\n",
    "    for s in tes_sess:\n",
    "        outseq = []\n",
    "        contain_cold = False\n",
    "        for i in tes_sess[s]:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                num_cold += 1\n",
    "                contain_cold = True\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "\n",
    "        if len(outseq) < 2:\n",
    "            print('obtain test session length is 1')\n",
    "            continue\n",
    "        if contain_cold:\n",
    "            test_seqs_cold += [outseq]\n",
    "        else:\n",
    "            test_seqs += [outseq]\n",
    "    print(\"#test_session: \",len(test_seqs))\n",
    "    print(\"#cold_test_session: \",len(test_seqs_cold))\n",
    "    print('#cold_items',str(num_cold))\n",
    "    return test_seqs, test_seqs_cold\n",
    "\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs, iprice, icate):\n",
    "    out_seqs = []\n",
    "    out_price = []\n",
    "    out_cate = []\n",
    "    labs = []\n",
    "    max_length = 19\n",
    "    for seq, pri, rat, cat in zip(iseqs, iprice, icate):\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "            out_price += [pri[:-i]]\n",
    "            out_cate += [cat[:-i]]\n",
    "    return out_seqs, out_price, out_cate, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    print(\"no data augment\")\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    max_length = 19\n",
    "    for seq in iseqs:     \n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "tra_seqs = obtian_tra()\n",
    "tes_seqs  = obtian_tes()\n",
    "old2new_nocold_path = './dict/' + datasets_name + '/old2newNocold.csv'\n",
    "old_list_nocold = list(item_dict.keys())\n",
    "new_list_nocold = list(item_dict.values())\n",
    "old2new_nocold_dict = {'old':old_list_nocold, 'new':new_list_nocold}\n",
    "data_old_new_no = pd.DataFrame(old2new_nocold_dict)\n",
    "data_old_new_no.to_csv(old2new_nocold_path)\n",
    "\n",
    "\n",
    "tr_seqs,  tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs,  te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "\n",
    "print('train sequence: ',tr_seqs[:5])\n",
    "print('train lab: ',tr_labs[:5])\n",
    "\n",
    "\n",
    "# 增加 all_item_price_list & all_item_rating_list\n",
    "\n",
    "# 共现关系\n",
    "fre_seq = tra_seqs + tes_seqs\n",
    "fre_mat = np.zeros((item_ctr-1, item_ctr-1), dtype=int)\n",
    "for sess in fre_seq:\n",
    "    for it1 in sess:\n",
    "        for it2 in sess:\n",
    "            if it1 != it2:\n",
    "                fre_mat[it1-1, it2-1] +=1 \n",
    "\n",
    "\n",
    "fre_mat_norm = fre_mat/(np.sum(fre_mat, axis= 1)+1e-8)[:, np.newaxis]\n",
    "\n",
    "\n",
    "indices = []\n",
    "all_data = []\n",
    "all_item_pair = []\n",
    "non_co = 0\n",
    "for i_list in fre_mat_norm:\n",
    "    i_temp_list = np.nonzero(i_list)[0].tolist()\n",
    "    if len(i_temp_list) == 0:\n",
    "        non_co += 1\n",
    "    indices += i_temp_list\n",
    "    all_data += i_list[np.nonzero(i_list)].tolist()\n",
    "    all_item_pair.append(i_temp_list)\n",
    "print('item without co-occurren: ',str(non_co))\n",
    "indptr = []\n",
    "indptr.append(0)\n",
    "for j in range(len(all_item_pair)):\n",
    "    session = np.unique(all_item_pair[j]) #统计session中不同item，去重，并按照item_id排序\n",
    "    length = len(session)\n",
    "    s = indptr[-1]\n",
    "    indptr.append((s + length))\n",
    "# indptr:session长度累加和; indices:item_id 减1, 由每个session内item组成; data:item在session内的权重，全部为1.\n",
    "coo_mat = (all_data, indices, indptr)\n",
    "\n",
    "\n",
    "indentify_mat = np.identity(fre_mat.shape[0],dtype=int)\n",
    "fre_mat = fre_mat - indentify_mat\n",
    "# 每行内，从大到小排序\n",
    "arg_mat = np.argsort(-fre_mat, axis = 1)\n",
    "pos_count =  np.sort(-fre_mat, axis=1)[:, :num_pos].tolist()\n",
    "\n",
    "# 共现最多与最少的序号，1序\n",
    "pos_index = (arg_mat[:, :num_pos] + 1).tolist()\n",
    "\n",
    "neg_index = (arg_mat[:, -num_neg-1:-1] + 1).tolist()\n",
    "\n",
    "pos_neg_list = []\n",
    "pos_neg_list.append(pos_index)\n",
    "pos_neg_list.append(neg_index)\n",
    "pos_neg_list.append(pos_count)\n",
    "\n",
    "item_pairs_dict = {}\n",
    "for item_id, idx_list, num_list in zip(range(1,item_ctr), pos_index, pos_count):\n",
    "    if item_id not in item_pairs_dict:\n",
    "        item_pairs_dict[item_id] = []\n",
    "    for idx_i, num_i in zip(idx_list, num_list):\n",
    "        if num_i != 0:\n",
    "            item_pairs_dict[item_id].append(idx_i)\n",
    "\n",
    "tr_flag = []\n",
    "for sess_temp, lab_temp in zip(tr_seqs, tr_labs):\n",
    "    flag = -1\n",
    "    for ix_temp in list(set(sess_temp)):\n",
    "        if lab_temp in item_pairs_dict[ix_temp]:\n",
    "            flag = 1\n",
    "    tr_flag.append(flag)\n",
    "print('id, train, co-occurence sess: ',str(tr_flag.count(1)))\n",
    "print('Modality, train, non co-occurence sess: ',str(tr_flag.count(-1)))\n",
    "\n",
    "\n",
    "te_flag = []\n",
    "for sess_temp, lab_temp in zip(te_seqs, te_labs):\n",
    "    flag = 0\n",
    "    for ix_temp in list(set(sess_temp)):\n",
    "        if lab_temp in item_pairs_dict[ix_temp]:\n",
    "            flag = 1\n",
    "    te_flag.append(flag)\n",
    "print('id, test, co-occurence sess: ',str(tr_flag.count(1)))\n",
    "print('Modality, test, non co-occurence sess: ',str(tr_flag.count(-1)))\n",
    "\n",
    "tra = (tr_seqs, tr_flag, tr_labs, coo_mat)\n",
    "tes = (te_seqs, te_flag, te_labs, coo_mat)\n",
    "\n",
    "\n",
    "all_train = 0\n",
    "all_tes = 0\n",
    "for seq in tra_seqs:\n",
    "    all_train += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all_tes += len(seq)\n",
    "print('#train_interactions: ',all_train)\n",
    "print('#test_interactions: ',all_tes)\n",
    "\n",
    "\n",
    "print('#train_session: ',(len(tra_seqs)))\n",
    "print('#test_session: ',(len(tes_seqs)))\n",
    "\n",
    "\n",
    "print('sequence average length: ', (all_train+all_tes)/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "train_data_path = './datasets/' + datasets_name\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_normal_test = train_data_path + \"/test.txt\"\n",
    "path_frequence = train_data_path + \"/frequence.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_normal_test, 'wb'))\n",
    "pickle.dump(pos_neg_list, open(path_frequence, 'wb'))\n",
    "\n",
    "\n",
    "# get asin item_id 获得asin list 有顺序, 给处理 img及text\n",
    "asin2old_path = 'dict/' + datasets_name +'/asin2itemID.csv'\n",
    "old2new_path = 'dict/' + datasets_name +'/old2newNocold.csv'\n",
    "\n",
    "\n",
    "asin2old = pd.read_csv(asin2old_path)\n",
    "asin2old = asin2old.rename(columns={ 'itemID':'old'})\n",
    "asin2old = asin2old[['asin', 'old']]\n",
    "\n",
    "\n",
    "old2new = pd.read_csv(old2new_path)\n",
    "old2new = old2new[['old', 'new']]\n",
    "\n",
    "\n",
    "asin2new = pd.merge(asin2old, old2new, how='left', on = 'old')\n",
    "\n",
    "\n",
    "asin2new = asin2new.dropna(axis=0)\n",
    "asin2ItemID = asin2new.reset_index()[['asin', 'new']]\n",
    "\n",
    "asin2ItemID.sort_values(by=[\"new\"],inplace=True,ascending=[True])\n",
    "\n",
    "\n",
    "asin_list = asin2ItemID['asin'].tolist()\n",
    "asin_list_save = train_data_path + '/asinlist.npy'\n",
    "np.save(asin_list_save, asin_list)\n",
    "\n",
    "\n",
    "\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"data formulation: id_seq,labs\")\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NARM, S-POP, SR-GNN, DHCN, AttenMixer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 14651\n",
      "train sequence:  81516\n",
      "test sequence:  8976\n",
      "train sample:  [[1], [3], [4]] [2, 3, 5]\n",
      "test sample:  [[6842], [4504, 3161, 3161, 676], [677]] [9767, 9087, 677]\n",
      "sequence average length:  3.1174247447288157\n",
      "dataset:  Sports_and_Outdoors\n",
      "2023-08-20 18:48:12\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "datasets_name = 'Sports_and_Outdoors'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    return test_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "\n",
    "tra_seqs = obtian_tra()\n",
    "tes_seqs = obtian_tes()\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "tra = (tr_seqs, tr_labs)\n",
    "tes = (te_seqs, te_labs)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'NARM/' + datasets_name\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COTREC & DGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 14651\n",
      "train sequence:  81516\n",
      "test sequence:  8976\n",
      "train sample:  [[1], [3], [4]] [2, 3, 5]\n",
      "test sample:  [[6842], [4504, 3161, 3161, 676], [677]] [9767, 9087, 677]\n",
      "all train sequences: [[1, 2], [3, 3], [4, 5]]\n",
      "sequence average length:  3.1174247447288157\n",
      "dataset:  Sports_and_Outdoors\n",
      "COTREC&DGNN\n",
      "2023-08-20 18:53:41\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Sports_and_Outdoors'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    return test_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "tra_seqs = obtian_tra()\n",
    "tes_seqs = obtian_tes()\n",
    "\n",
    "all_train_seqs = tra_seqs + tes_seqs\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "tra = (tr_seqs, tr_labs)\n",
    "tes = (te_seqs, te_labs)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "print(\"all train sequences:\", all_train_seqs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'COTREC&DGNN/' + datasets_name + 'Disen'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "path_data_all = train_data_path+ \"/all_train_seq.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "pickle.dump(all_train_seqs, open(path_data_all, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"COTREC&DGNN\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LESSR & MSGIFSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 14651\n",
      "train sequence:  81516\n",
      "test sequence:  8976\n",
      "train sample:  [[1, 2], [3, 3], [4, 5]]\n",
      "test sample:  [[6842, 9767], [4504, 3161, 3161, 676, 9087], [677, 677]]\n",
      "sequence average length:  3.1174247447288157\n",
      "save item_num : 14650\n",
      "dataset:  Sports_and_Outdoors\n",
      "2023-08-20 18:54:40\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "datasets_name = 'Sports_and_Outdoors'\n",
    "# data_path = 'data_time_interval/' + datasets_name +'_data.csv'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "    return test_seqs\n",
    "\n",
    "\n",
    "tra_seqs = obtian_tra()\n",
    "tes_seqs = obtian_tes()\n",
    "\n",
    "print(\"train sequence: \", len(tra_seqs))\n",
    "print(\"test sequence: \", len(tes_seqs))\n",
    "print(\"train sample: \", tra_seqs[:3])\n",
    "print(\"test sample: \", tes_seqs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "# train_data_path = 'LESSR/Cell_Phones_and_Accessories'\n",
    "train_data_path = 'LESSRDisen/' + datasets_name + 'Disen'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "path_item_num = train_data_path + '/num_items.txt'\n",
    "\n",
    "print('save item_num :', len(item_dict))\n",
    "pickle.dump(len(item_dict)+1, open(path_item_num, 'wb'))\n",
    "pickle.dump(tra_seqs, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes_seqs, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT4Rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train_items 14651\n",
      "train sequence:  81516\n",
      "test sequence:  8976\n",
      "train sample:  [[1], [3], [4]] [2, 3, 5]\n",
      "test sample:  [[6842], [4504, 3161, 3161, 676], [677]] [9767, 9087, 677]\n",
      "sequence average length:  3.1174247447288157\n",
      "dataset:  Sports_and_Outdoors\n",
      "2023-08-20 18:55:19\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "datasets_name = 'Sports_and_Outdoors'\n",
    "# data_path = 'data_time_interval/' + datasets_name +'_data.csv'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID']]\n",
    "\n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    sessid_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "        sessid_seqs += [s]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs, sessid_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    test_sessid_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "        test_sessid_seqs += [s]\n",
    "    return test_seqs, test_sessid_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "\n",
    "tra_seqs, tra_sessid = obtian_tra()\n",
    "tes_seqs, tes_sessid = obtian_tes()\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "tra = (tra_sessid, tr_seqs, tr_labs)\n",
    "tes = (tes_sessid, te_seqs, te_labs)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'Bert4RecDisen/' + datasets_name + 'Disen'\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(\"BERT4Rec\")\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******SKNN******\n",
      "#train_items 14651\n",
      "train sequence:  81516\n",
      "test sequence:  8976\n",
      "train sample:  [[1], [3], [4]] [2, 3, 5]\n",
      "test sample:  [[6842], [4504, 3161, 3161, 676], [677]] [9767, 9087, 677]\n",
      "sequence average length:  3.1174247447288157\n",
      "dataset:  Sports_and_Outdoors\n",
      "2023-08-20 18:56:38\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets_name = 'Sports_and_Outdoors'\n",
    "# data_path = 'data_time_interval/' + datasets_name +'_data.csv'\n",
    "data_path = 'dict/' + datasets_name +'/session_data.csv'\n",
    "\n",
    "\n",
    "print(\"******SKNN******\")\n",
    "\n",
    "data_all = pd.read_csv(data_path)\n",
    "data = data_all[['sessionID', 'itemID', 'time']]\n",
    "\n",
    " \n",
    "\n",
    "# dict (sessionID:[itemID,itemID])\n",
    "sess_all = {}\n",
    "sess_date = {}\n",
    "for _, row in data.iterrows():\n",
    "    sess_id = row['sessionID']\n",
    "    item_id = row['itemID']\n",
    "    timestamp = row['time']\n",
    "    if sess_id in sess_all:\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        sess_date[sess_id] = timestamp\n",
    "    else:\n",
    "        sess_all[sess_id] = []\n",
    "        sess_all[sess_id].append(item_id)\n",
    "        sess_date[sess_id] = timestamp\n",
    "\n",
    "\n",
    "sess_total = data['sessionID'].max()\n",
    "split_num = int(sess_total/10*9)\n",
    "\n",
    "tra_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "tes_sess = dict() # dict(session_id:[item_id,item_id])\n",
    "for sess_temp in sess_all.keys():\n",
    "    all_seqs = sess_all[sess_temp]\n",
    "    if len(all_seqs) < 2:\n",
    "        continue\n",
    "    if len(all_seqs) > 20:\n",
    "        all_seqs = all_seqs[:20]\n",
    "    if int(sess_temp) < split_num:\n",
    "        tra_sess[sess_temp] = all_seqs\n",
    "    else:\n",
    "        tes_sess[sess_temp] = all_seqs\n",
    "\n",
    "\n",
    "item_dict = {}#dict(old_itemID: new_itemID)\n",
    "# Convert training sessions to sequences and renumber items to start from 1\n",
    "def obtian_tra():\n",
    "    train_seqs = []\n",
    "    sessid_seqs = []\n",
    "    time_seqs = []\n",
    "    item_ctr = 1\n",
    "    for s in tra_sess:\n",
    "        seq = tra_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2:  # Doesn't occur\n",
    "            continue\n",
    "        train_seqs += [outseq]\n",
    "        sessid_seqs += [s]\n",
    "        time_seqs += [sess_date[s]]\n",
    "    print(\"#train_items\",item_ctr)    \n",
    "    return train_seqs, sessid_seqs, time_seqs\n",
    "\n",
    "\n",
    "# Convert test sessions to sequences, ignoring items that do not appear in training set\n",
    "def obtian_tes():\n",
    "    test_seqs = []\n",
    "    test_sessid_seqs = []\n",
    "    test_time_seqs = []\n",
    "    for s in tes_sess:\n",
    "        seq = tes_sess[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_seqs += [outseq]\n",
    "        test_sessid_seqs += [s]\n",
    "        test_time_seqs += [sess_date[s]]\n",
    "    return test_seqs, test_sessid_seqs, test_time_seqs\n",
    "\n",
    "# 数据增强\n",
    "def process_seqs(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "# 无数据增强\n",
    "def process_seqs_no(iseqs):\n",
    "    out_seqs = []\n",
    "    labs = []\n",
    "    for seq in iseqs:\n",
    "        labs += [seq[-1]]\n",
    "        out_seqs += [seq[:-1]]\n",
    "    return out_seqs, labs\n",
    "\n",
    "\n",
    "tra_seqs, tra_sessid, tra_time = obtian_tra()\n",
    "tes_seqs, tes_sessid, tes_time = obtian_tes()\n",
    "\n",
    "tr_seqs, tr_labs = process_seqs_no(tra_seqs)\n",
    "te_seqs, te_labs = process_seqs_no(tes_seqs)\n",
    "\n",
    "tra = (tra_sessid, tr_seqs, tr_labs, tra_time)\n",
    "tes = (tes_sessid, te_seqs, te_labs, tes_time)\n",
    "print(\"train sequence: \", len(tr_seqs))\n",
    "print(\"test sequence: \", len(te_seqs))\n",
    "print(\"train sample: \", tr_seqs[:3], tr_labs[:3])\n",
    "print(\"test sample: \", te_seqs[:3], te_labs[:3])\n",
    "all = 0\n",
    "\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('sequence average length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "train_data_path = 'SKNNDisen/' + datasets_name\n",
    "\n",
    "if not os.path.exists(train_data_path):\n",
    "    os.makedirs(train_data_path)\n",
    "path_data_train = train_data_path + \"/train.txt\"\n",
    "path_data_test = train_data_path + \"/test.txt\"\n",
    "\n",
    "pickle.dump(tra, open(path_data_train, 'wb'))\n",
    "pickle.dump(tes, open(path_data_test, 'wb'))\n",
    "print(\"dataset: \", datasets_name)\n",
    "print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
